{"Meta-Llama-3-8B-Instruct (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": -0.013059589222282703, "spearman": -0.013059589222282656, "kendall": -0.013059589222282654}, "p_value": {"pearson": 0.8217738872533862, "spearman": 0.8217738872533847, "kendall": 0.8213403259210199}, "kappa_score": -0.010145181039006301, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mistral-7B-Instruct-v0.2 (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": -0.040541921712347134, "spearman": -0.04054192171234707, "kendall": -0.040541921712347065}, "p_value": {"pearson": 0.4842003366269822, "spearman": 0.4842003366269828, "kendall": 0.4832809477024449}, "kappa_score": -0.0246201052016366, "total_responses": 300, "valid_responses": 295, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Starling-LM-7B-alpha (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.015791098494992103, "spearman": 0.01579109849499207, "kendall": 0.01579109849499207}, "p_value": {"pearson": 0.7853259903322266, "spearman": 0.7853259903322234, "kendall": 0.7848119991989516}, "kappa_score": 0.01297497683039861, "total_responses": 300, "valid_responses": 14, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "c4ai-command-r-v01 (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": -0.12423115207712183, "spearman": -0.1242311520771219, "kendall": -0.12423115207712192}, "p_value": {"pearson": 0.031468029469192985, "spearman": 0.031468029469192714, "kendall": 0.031701250829579246}, "kappa_score": -0.11862036821253774, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "OLMo-7B-Instruct (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.039548350910666386, "spearman": 0.03954835091066649, "kendall": 0.0395483509106665}, "p_value": {"pearson": 0.4949829488735026, "spearman": 0.49498294887350214, "kendall": 0.494066785330705}, "kappa_score": 0.023578764646318695, "total_responses": 300, "valid_responses": 281, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Meta-Llama-3-70B-Instruct (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.09866630782239498, "spearman": 0.09866630782239508, "kendall": 0.09866630782239509}, "p_value": {"pearson": 0.08801157534592774, "spearman": 0.08801157534592763, "kendall": 0.08798947134139531}, "kappa_score": 0.07600208224882876, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gpt-3.5-turbo-0125 (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.11620026944469597, "spearman": 0.11620026944469582, "kendall": 0.11620026944469579}, "p_value": {"pearson": 0.04431880498852842, "spearman": 0.04431880498852871, "kendall": 0.04450633560583367}, "kappa_score": 0.02664522797748936, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gemini-1.5-flash-latest (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.23004961751511202, "spearman": 0.23004961751511252, "kendall": 0.2300496175151125}, "p_value": {"pearson": 5.770196824360233e-05, "spearman": 5.770196824360139e-05, "kendall": 6.951790127636946e-05}, "kappa_score": 0.22432898300911097, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "gpt-4o (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.4224625387437791, "spearman": 0.4224625387437788, "kendall": 0.42246253874377887}, "p_value": {"pearson": 2.0454122610185482e-14, "spearman": 2.0454122610186245e-14, "kendall": 2.7714265080585833e-13}, "kappa_score": 0.41898634453781514, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.09549558753434358, "spearman": 0.09549558753434369, "kendall": 0.09549558753434367}, "p_value": {"pearson": 0.09875859380015985, "spearman": 0.09875859380015886, "kendall": 0.09868282782508439}, "kappa_score": 0.05904059040590415, "total_responses": 300, "valid_responses": 298, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "Mixtral-8x22B-Instruct-v0.1 (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": 0.03036281475502689, "spearman": 0.030362814755026966, "kendall": 0.030362814755026963}, "p_value": {"pearson": 0.6004004642795211, "spearman": 0.6004004642795193, "kendall": 0.5995677947804869}, "kappa_score": 0.017190362124841485, "total_responses": 300, "valid_responses": 300, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}, "c4ai-command-r-plus (SP: None, AP: 1)": {"Sound Reasoning": {"corr_coeff": {"pearson": -0.01732870321943618, "spearman": -0.017328703219436244, "kendall": -0.017328703219436244}, "p_value": {"pearson": 0.7650076522782606, "spearman": 0.7650076522782602, "kendall": 0.764450785646628}, "kappa_score": -0.016009852216748888, "total_responses": 300, "valid_responses": 292, "krippendorff_alpha": 1, "type": "categorical", "expert": "true", "task": "Reasoning"}}}