{"Meta-Llama-3-8B-Instruct (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.34829201098937135, "spearman": 0.341393889506189, "kendall": 0.27997302120076556}, "p_value": {"pearson": 0.000383776884211609, "spearman": 0.0005088278588933205, "kendall": 0.0005383914862768892}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Mistral-7B-Instruct-v0.2 (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.3385981424041889, "spearman": 0.40123817860354327, "kendall": 0.31811914342007586}, "p_value": {"pearson": 0.0005694064890046442, "spearman": 3.5161417589978036e-05, "kendall": 6.01020023449321e-05}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 98, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Starling-LM-7B-alpha (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.12296769650243143, "spearman": 0.1284606575244697, "kendall": 0.10731019208540299}, "p_value": {"pearson": 0.22290162116856702, "spearman": 0.20275536718720935, "kendall": 0.19898494483550921}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 95, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "c4ai-command-r-v01 (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.4632272196015049, "spearman": 0.5253916722249533, "kendall": 0.4238828463822977}, "p_value": {"pearson": 1.2155158199502093e-06, "spearman": 1.995801015165169e-08, "kendall": 4.24786896967137e-08}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 93, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "OLMo-7B-Instruct (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.019670497838102215, "spearman": 0.0027021578009442187, "kendall": 0.0026513852014814912}, "p_value": {"pearson": 0.8459795947501499, "spearman": 0.9787134540286087, "kendall": 0.9724942736014839}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 79, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Meta-Llama-3-70B-Instruct (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.547374483094279, "spearman": 0.6001512381604366, "kendall": 0.4907150991296925}, "p_value": {"pearson": 3.790488529731955e-09, "spearman": 4.133669103443195e-11, "kendall": 2.9690959388689884e-10}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "gpt-3.5-turbo-0125 (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.49658326169088784, "spearman": 0.48798773066218554, "kendall": 0.3946527917765272}, "p_value": {"pearson": 1.4832926681485172e-07, "spearman": 2.605979440035481e-07, "kendall": 4.22692628160292e-07}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "gemini-1.5-flash-latest (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.6036473047727718, "spearman": 0.5900751012389175, "kendall": 0.4710433994507182}, "p_value": {"pearson": 2.9751043283148624e-11, "spearman": 1.04351553522608e-10, "kendall": 1.3653651168260718e-09}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "gpt-4o (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.7090161407521609, "spearman": 0.6551622337452909, "kendall": 0.5258555091409866}, "p_value": {"pearson": 1.529340302207796e-16, "spearman": 1.4111924438992228e-13, "kendall": 1.8267054707324947e-11}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Mixtral-8x7B-Instruct-v0.1 (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.45999114169074795, "spearman": 0.5580325993542875, "kendall": 0.43428333506289435}, "p_value": {"pearson": 1.473514148850341e-06, "spearman": 1.6210603833596942e-09, "kendall": 1.5702883572060616e-08}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 99, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "Mixtral-8x22B-Instruct-v0.1 (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.6851812148058457, "spearman": 0.6284483869866779, "kendall": 0.51430607054368}, "p_value": {"pearson": 3.760882678274499e-15, "spearman": 2.5609022177722652e-12, "kendall": 2.2672198605033853e-10}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}, "c4ai-command-r-plus (SP: None, AP: 1)": {"acceptability": {"corr_coeff": {"pearson": 0.32709904806495027, "spearman": 0.36459375392060456, "kendall": 0.30213540738437095}, "p_value": {"pearson": 0.0008945841435877097, "spearman": 0.00019199581057422098, "kendall": 0.0002631298852709692}, "kappa_score": NaN, "total_responses": 100, "valid_responses": 100, "krippendorff_alpha": 0.5718833015222707, "type": "graded", "expert": "false", "task": "Acceptability"}}}